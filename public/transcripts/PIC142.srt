1
00:00:00,000 --> 00:00:01,490
Dilla verità,

2
00:00:01,490 --> 00:00:05,468
anche tu qualche volta hai utilizzato un chatbot o un motore

3
00:00:05,468 --> 00:00:09,446
di ricerca basato su intelligenza artificiale generativa per

4
00:00:09,446 --> 00:00:13,489
farti dare delle risposte o dei consigli su qualche argomento?

5
00:00:13,490 --> 00:00:20,059
Magari in un ambito nel quale non hai troppa
dimestichezza o senti il bisogno di una guida.

6
00:00:20,059 --> 00:00:21,489
Non c'è niente di male,

7
00:00:21,489 --> 00:00:24,280
ovviamente lo facciamo tutti io per primo.

8
00:00:24,280 --> 00:00:30,035
Ma c'è una cosa da tenere bene a mente quando utilizziamo un large language

9
00:00:30,035 --> 00:00:35,487
model e cioè che esso non solo potrebbe facilmente commettere errori ma

10
00:00:35,487 --> 00:00:39,955
potrebbe anche mentire o addirittura assecondare le nostre

11
00:00:39,955 --> 00:00:44,119
affermazioni anche se queste sono completamente errate.

12
00:00:44,119 --> 00:00:47,049
Oggi scopriamo insieme il motivo per cui,

13
00:00:47,049 --> 00:00:53,640
nonostante questi strumenti possano rappresentare
un enorme vantaggio per chi li utilizza,

14
00:00:53,640 --> 00:00:59,289
è bene sapere che non bisogna mai fidarsi ciecamente delle loro risposte.

15
00:00:59,289 --> 00:01:04,709
Parleremo infatti del perché le intelligenze artificiali inventano cose,

16
00:01:04,709 --> 00:01:07,400
ci assecondano e ci mentono.

17
00:01:07,400 --> 00:01:08,040
Sigla!

18
00:01:12,250 --> 00:01:14,329
Benvenuti su Pensieri in codice,

19
00:01:14,329 --> 00:01:16,250
il podcast dove si ragiona da informatici.

20
00:01:16,250 --> 00:01:18,250
Con Valerio Galano.

21
00:01:23,250 --> 00:01:25,709
Prima di iniziare con il nostro discorso,

22
00:01:25,709 --> 00:01:28,449
trovo doveroso fare alcuni chiarimenti.

23
00:01:28,450 --> 00:01:29,570
Innanzitutto,

24
00:01:29,569 --> 00:01:35,769
anche se in questo episodio utilizzeremo la
definizione generica di "intelligenza artificiale",

25
00:01:35,769 --> 00:01:41,879
ricordiamoci sempre che si parlerà solo ed
esclusivamente di machine learning generativo,

26
00:01:41,879 --> 00:01:44,609
nello specifico di large language model.

27
00:01:44,609 --> 00:01:46,069
In secondo luogo,

28
00:01:46,069 --> 00:01:48,839
e questo non lo dirò mai abbastanza,

29
00:01:48,840 --> 00:01:51,340
anche se utilizzeremo verbi come mentire,

30
00:01:51,340 --> 00:01:52,250
assecondare,

31
00:01:52,250 --> 00:01:53,049
decidere,

32
00:01:53,049 --> 00:01:53,849
imparare,

33
00:01:53,849 --> 00:01:54,719
eccetera,

34
00:01:54,719 --> 00:02:02,689
dobbiamo avere sempre ben chiaro in mente che gli LLM
non hanno alcuna intenzionalità nei loro comportamenti,

35
00:02:02,689 --> 00:02:07,289
né tantomeno una vera comprensione del significato dei testi che generano.

36
00:02:07,289 --> 00:02:09,750
Quando parliamo di allucinazioni,

37
00:02:09,750 --> 00:02:11,500
di pensiero o di menzogne,

38
00:02:11,500 --> 00:02:17,180
lo facciamo solo perché sono concetti che
ci risultano più facili da comprendere,

39
00:02:17,180 --> 00:02:23,569
e descrivono funzionamenti di queste macchine che
assomigliano ai relativi comportamenti tipici degli umani.

40
00:02:23,569 --> 00:02:26,919
Ma stiamo sempre parlando di macchine,

41
00:02:26,920 --> 00:02:28,250
non di umani.

42
00:02:28,250 --> 00:02:29,110
Pertanto,

43
00:02:29,110 --> 00:02:37,090
seppur apparendo in tutto e per tutto simili
all'interazione con un altro individuo della nostra specie,

44
00:02:37,090 --> 00:02:43,689
le ragioni dietro alla generazione di ogni
singola risposta sono completamente diverse.

45
00:02:43,689 --> 00:02:44,750
In realtà,

46
00:02:44,750 --> 00:02:47,340
lo ripetiamo ancora una volta,

47
00:02:47,340 --> 00:02:51,689
anche se ci ostiniamo a chiamarli "intelligenze",

48
00:02:51,689 --> 00:02:54,599
questi algoritmi non sono esseri intelligenti,

49
00:02:54,599 --> 00:02:56,009
non pensano veramente,

50
00:02:56,009 --> 00:02:57,609
non hanno obiettivi,

51
00:02:57,610 --> 00:02:59,330
intenzioni o volontà propria.

52
00:02:59,329 --> 00:03:02,009
Se si comportano in un determinato modo,

53
00:03:02,009 --> 00:03:03,519
come vedremo a breve,

54
00:03:03,519 --> 00:03:04,769
è semplicemente,

55
00:03:04,769 --> 00:03:07,389
dove semplicemente va fra virgolette,

56
00:03:07,389 --> 00:03:09,370
per questioni statistiche.

57
00:03:09,370 --> 00:03:16,090
Siamo noi ad attribuire ai risultati dei
loro calcoli significati tipicamente umani.

58
00:03:21,500 --> 00:03:28,490
Se anche non ti sarà capitato di ricevere personalmente
risposte errate da parte di un large language model,

59
00:03:28,490 --> 00:03:37,820
avrai certamente letto qualche notizia in giro per il web su chatbot
che danno consigli assurdi o si comportano in modi inappropriati.

60
00:03:37,819 --> 00:03:41,109
Quando l'output di un LLM non ha senso,

61
00:03:41,110 --> 00:03:43,410
si parla di allucinazione,

62
00:03:43,409 --> 00:03:47,968
e si intende quella condizione per cui il modello generativo

63
00:03:47,968 --> 00:03:51,854
produce una risposta che è perfettamente corretta e

64
00:03:51,854 --> 00:03:55,740
convincente dal punto di vista sintattico e formale,

65
00:03:55,740 --> 00:03:58,140
ma è sbagliata nel contenuto.

66
00:03:58,139 --> 00:03:59,089
Ad esempio,

67
00:03:59,090 --> 00:04:06,650
a me una volta è capitato che mentre facevo delle ricerche
su Charles Babbage per gli episodi su Ada Lovelace,

68
00:04:06,650 --> 00:04:07,340
un LLM,

69
00:04:07,340 --> 00:04:08,069
credo,

70
00:04:08,069 --> 00:04:09,009
chatgpt,

71
00:04:09,009 --> 00:04:15,009
mi abbia riportato una citazione molto bella attribuita all'inventore,

72
00:04:15,009 --> 00:04:23,939
ma che in realtà non trovava riscontro in nessuna fonte e
soprattutto era stranamente datata circa 40 anni dopo la sua morte.

73
00:04:23,939 --> 00:04:25,019
A proposito,

74
00:04:25,019 --> 00:04:29,199
se non hai ancora ascoltato la miniserie su Ada Lovelace,

75
00:04:29,199 --> 00:04:35,539
sappi che hai perso uno dei migliori contenuti che
io abbia mai prodotto qui su Pensieri in Codice,

76
00:04:35,540 --> 00:04:36,740
quindi recuperala,

77
00:04:36,740 --> 00:04:37,689
mi raccomando,

78
00:04:37,689 --> 00:04:39,389
te la metto in descrizione.

79
00:04:39,389 --> 00:04:41,909
Tornando alle nostre allucinazioni però,

80
00:04:41,909 --> 00:04:45,159
secondo fonti più oggettive come la stessa OpenAI,

81
00:04:45,159 --> 00:04:49,809
una delle maggiori aziende produttrici di large language model,

82
00:04:49,810 --> 00:05:02,420
negli ultimi 4 modelli di chatgpt sono presenti allucinazioni in determinati
ambiti in percentuali di risposte che oscillano tra il 37 e l'80%.

83
00:05:02,420 --> 00:05:07,410
Ciò vuol dire che anche con i migliori LLM in circolazione,

84
00:05:07,409 --> 00:05:12,699
una risposta su 3 potrebbe essere errata in parte o totalmente.

85
00:05:12,699 --> 00:05:18,459
Un dato abbastanza preoccupante se pensiamo a
come vengono utilizzati oggi questi strumenti.

86
00:05:18,459 --> 00:05:24,269
Riflettendo però un attimo sulle basi del
funzionamento dei large language model,

87
00:05:24,269 --> 00:05:31,060
non dovrebbe poi essere troppo complicato capire da
dove originano tali e tanto diffusi malfunzionamenti.

88
00:05:31,139 --> 00:05:35,930
Anche su questa questione c'è un interessantissimo episodio sempre di

89
00:05:35,930 --> 00:05:41,063
questo podcast intitolato "Come funziona chatgpt" in cui ho già affrontato

90
00:05:41,063 --> 00:05:46,060
l'argomento in modo più approfondito e trovi anche questo in descrizione.

91
00:05:46,060 --> 00:05:46,490
Ma,

92
00:05:46,490 --> 00:05:49,240
volendo riassumere brevemente,

93
00:05:49,240 --> 00:05:58,360
possiamo semplicemente dire che in fin dei conti un LLM
compone i propri testi accodando una parola dopo l'altra,

94
00:05:58,360 --> 00:06:03,659
selezionando ciascuna di esse di volta in volta su base statistica.

95
00:06:03,659 --> 00:06:04,610
In pratica,

96
00:06:04,610 --> 00:06:07,210
partendo da una sequenza di parole,

97
00:06:07,210 --> 00:06:11,685
individua statisticamente la successiva valutando quale è più

98
00:06:11,685 --> 00:06:16,665
probabile che possa accodarsi basandosi sulle occorrenze all'interno

99
00:06:16,665 --> 00:06:20,780
di un corpus di documenti con i quali è stato addestrato.

100
00:06:20,779 --> 00:06:26,979
Più volte nei dati di addestramento un gruppo
di parole è seguito da un'altra parola,

101
00:06:26,980 --> 00:06:27,340
più,

102
00:06:27,340 --> 00:06:28,690
secondo il modello,

103
00:06:28,689 --> 00:06:32,539
è probabile che quella parola si adatti bene al contesto.

104
00:06:32,539 --> 00:06:41,039
L'algoritmo poi replica questa operazione di scelta un certo numero
di volte fino a comporre un testo della lunghezza desiderata.

105
00:06:41,039 --> 00:06:51,099
Un LLM reale ovviamente è molto più complesso di così ed è dotato
di moltissime sovrastrutture utili a migliorarne la qualità.

106
00:06:51,100 --> 00:06:51,420
Ma,

107
00:06:51,419 --> 00:06:53,740
per qualsiasi modello esistente,

108
00:06:53,740 --> 00:06:57,180
se lo si scompone e se si scava abbastanza a fondo,

109
00:06:57,180 --> 00:07:01,399
il funzionamento di base è più o meno quello che ti ho descritto.

110
00:07:01,399 --> 00:07:03,259
E che vuol dire questo?

111
00:07:03,259 --> 00:07:04,529
Semplice,

112
00:07:04,529 --> 00:07:05,059
che,

113
00:07:05,060 --> 00:07:06,230
a livello logico,

114
00:07:06,230 --> 00:07:13,700
un LLM che potrebbe intuitivamente sembrare
un enorme database della conoscenza umana,

115
00:07:13,699 --> 00:07:18,319
in realtà è molto più simile ad una gigantesca palla otto.

116
00:07:18,319 --> 00:07:19,699
Hai presente?

117
00:07:19,699 --> 00:07:26,120
Quelle sfere piene di liquido che agiti e dopo
qualche istante fanno apparire una risposta?

118
00:07:26,120 --> 00:07:26,419
Sì?

119
00:07:26,419 --> 00:07:26,789
No?

120
00:07:26,789 --> 00:07:27,419
Forse?

121
00:07:27,419 --> 00:07:31,289
Se aprissimo un LLM e vi guardassimo all'interno,

122
00:07:31,289 --> 00:07:36,509
non vedremmo delle informazioni intese come dati ai quali viene dato un senso,

123
00:07:36,509 --> 00:07:39,459
ma solo miliardi e miliardi di numeri.

124
00:07:39,459 --> 00:07:43,170
Questi numeri vengono utilizzati dal modello per scegliere,

125
00:07:43,170 --> 00:07:44,300
di volta in volta,

126
00:07:44,300 --> 00:07:47,250
la parola successiva da aggiungere al testo,

127
00:07:47,250 --> 00:07:57,100
fra quelle che sempre nel corpus di documenti utilizzati ricorre con maggiore
frequenza di seguito a quelle che già compongono la prima parte del testo.

128
00:07:57,100 --> 00:07:58,310
All'atto pratico,

129
00:07:58,310 --> 00:08:00,430
volendo semplificare al massimo,

130
00:08:00,430 --> 00:08:04,660
ogni volta che il modello deve aggiungere una parola al testo,

131
00:08:04,660 --> 00:08:11,740
utilizza i numeri di qui sopra per calcolare dei
punteggi per ciascuna parola del suo vocabolario.

132
00:08:11,740 --> 00:08:17,819
Più alto è il punteggio e più è probabile che
la parola si adatti bene alle precedenti.

133
00:08:17,819 --> 00:08:20,939
Messa in questi termini sembra molto semplice,

134
00:08:20,939 --> 00:08:21,980
quasi banale,

135
00:08:21,980 --> 00:08:24,759
eppure essenzialmente questo meccanismo permette

136
00:08:24,759 --> 00:08:27,653
l'esistenza di quei potentissimi strumenti che noi

137
00:08:27,653 --> 00:08:30,660
oggi chiamiamo "intelligenze artificiali generative".

138
00:08:30,660 --> 00:08:31,470
Purtroppo,

139
00:08:31,470 --> 00:08:32,009
però,

140
00:08:32,009 --> 00:08:35,610
in questo processo c'è un limite intrinseco.

141
00:08:35,610 --> 00:08:38,690
Se il calcolo della parola successiva,

142
00:08:38,690 --> 00:08:39,620
infatti,

143
00:08:39,620 --> 00:08:42,399
fosse sempre preciso al 100%,

144
00:08:42,399 --> 00:08:45,200
a parità di modello utilizzato,

145
00:08:45,200 --> 00:08:49,820
ad una specifica frase seguirebbe sempre la stessa parola.

146
00:08:49,819 --> 00:08:51,529
Il calcolo del punteggio,

147
00:08:51,529 --> 00:08:52,190
infatti,

148
00:08:52,190 --> 00:08:58,740
darebbe sempre gli stessi risultati e questi
porterebbero ad avere sempre lo stesso vincitore.

149
00:08:58,740 --> 00:08:59,690
In pratica,

150
00:08:59,690 --> 00:09:01,279
ciò vuol dire che,

151
00:09:01,360 --> 00:09:02,670
se fosse come detto,

152
00:09:02,670 --> 00:09:04,980
accadrebbe che a prompt identici,

153
00:09:04,980 --> 00:09:09,980
uno specifico modello risponderebbe sempre con risposte identiche.

154
00:09:09,980 --> 00:09:14,139
Ma noi sappiamo benissimo che nella realtà non è così.

155
00:09:14,139 --> 00:09:17,940
Se facciamo 100 volte la stessa domanda allo stesso modello,

156
00:09:17,940 --> 00:09:20,990
otteniamo sempre una risposta leggermente diversa.

157
00:09:20,990 --> 00:09:23,639
Magari le informazioni cambiano solo un po',

158
00:09:23,639 --> 00:09:27,529
ma sicuramente cambia la forma in cui sono espresse.

159
00:09:27,529 --> 00:09:36,269
Questo comportamento deriva dal fatto che un LLM deve poter
rispondere a prompt che non conosce già perfettamente,

160
00:09:36,269 --> 00:09:46,079
e per fare ciò è necessario che esso possa combinare parole e frasi
in modi nuovi rispetto ai testi originali su cui è stato addestrato.

161
00:09:46,079 --> 00:09:57,980
Un modello linguistico che avesse bisogno di input precisi e restituisse output
costanti in modo deterministico non differirebbe da un software tradizionale,

162
00:09:57,980 --> 00:10:01,805
pertanto non avrebbe motivo di destare tanto entusiasmo

163
00:10:01,805 --> 00:10:05,767
ed essere oggetto di tanto dispendio di risorse a livello

164
00:10:05,767 --> 00:10:09,319
planetario come invece accade per questa tecnologia.

165
00:10:09,319 --> 00:10:16,269
È necessario invece che un LLM possa mescolare
concetti e idee provenienti da fonti diverse,

166
00:10:16,269 --> 00:10:22,360
che possa variare nell'utilizzo delle parole
e del modo di esprimere le informazioni,

167
00:10:22,360 --> 00:10:27,839
e tutto questo al fine di fornire risposte che sembrino creative e innovative.

168
00:10:27,839 --> 00:10:28,569
Il punto,

169
00:10:28,569 --> 00:10:29,139
però,

170
00:10:29,139 --> 00:10:40,440
è che un tale risultato di simulata creatività si può ottenere solo inserendo
nel processo di selezione delle parole una piccola percentuale di imprecisione,

171
00:10:40,440 --> 00:10:48,540
una leggera componente di casualità che permetta di non
associare sempre lo stesso output allo stesso input.

172
00:10:48,539 --> 00:10:54,549
La parola selezionata di volta in volta
quindi non è esattamente la più pertinente,

173
00:10:54,549 --> 00:10:58,559
ma una scelta all'interno del gruppo delle più pertinenti.

174
00:10:58,559 --> 00:11:05,589
E una così semplice accortezza è in realtà il
segreto della potenza di questi strumenti,

175
00:11:05,589 --> 00:11:10,240
è ciò che permette loro di fornire risposte sorprendenti,

176
00:11:10,240 --> 00:11:13,129
di mescolare idee da contesti diversi,

177
00:11:13,129 --> 00:11:16,899
di esprimersi con stili differenti e via discorrendo.

178
00:11:16,899 --> 00:11:18,039
Peccato,

179
00:11:18,039 --> 00:11:18,679
però,

180
00:11:18,680 --> 00:11:24,700
che per questa stessa identica ragione essi non possano essere accurati al 100%,

181
00:11:24,700 --> 00:11:25,810
ovviamente,

182
00:11:25,809 --> 00:11:37,359
e in determinati casi un susseguirsi di scelte un po' troppo creative porti
alla generazione di quelle risposte sbagliate che noi chiamiamo allucinazioni.

183
00:11:37,360 --> 00:11:42,093
Non esiste alcun modo per assicurarsi che parole messe in fila con una

184
00:11:42,093 --> 00:11:46,626
seppur piccola percentuale di casualità portino a comporre un testo

185
00:11:46,626 --> 00:11:51,159
con un senso logico ben definito e ad esporre informazioni corrette.

186
00:11:51,160 --> 00:11:51,840
Gli LLM,

187
00:11:51,839 --> 00:11:54,099
ripetiamolo ancora una volta,

188
00:11:54,100 --> 00:11:57,539
non hanno comprensione dei testi che generano,

189
00:11:57,539 --> 00:12:07,250
e pertanto si può tranquillamente affermare che essi non possano avere
capacità o intenzionalità di restituire affermazioni vere o false,

190
00:12:07,250 --> 00:12:08,019
di fatto,

191
00:12:08,019 --> 00:12:10,899
essi non hanno alcun vincolo di realtà.

192
00:12:10,899 --> 00:12:11,820
In effetti,

193
00:12:11,820 --> 00:12:14,580
guardandola da questo punto di vista,

194
00:12:14,580 --> 00:12:21,850
dovrebbe apparire chiaro che qualsiasi prodotto di
un modello generativo è di fatto un'allucinazione.

195
00:12:21,850 --> 00:12:22,679
Il modello,

196
00:12:22,679 --> 00:12:23,490
infatti,

197
00:12:23,490 --> 00:12:28,779
non è in grado di giudicare in alcun modo se
ciò che ha restituito sia sensato o meno.

198
00:12:28,860 --> 00:12:31,840
Siamo noi a dare un senso a questi testi,

199
00:12:31,840 --> 00:12:34,730
e se li troviamo corretti e sensati,

200
00:12:34,730 --> 00:12:36,399
li definiamo risposte,

201
00:12:36,399 --> 00:12:39,049
altrimenti li chiamiamo allucinazioni.

202
00:12:39,049 --> 00:12:41,080
Per completezza di discorso,

203
00:12:41,080 --> 00:12:41,480
poi,

204
00:12:41,480 --> 00:12:44,440
diciamo che ci sono anche altre ragioni,

205
00:12:44,440 --> 00:12:46,590
magari molto più intuitive,

206
00:12:46,590 --> 00:12:50,610
che possono portare i modelli generativi a sbagliare,

207
00:12:50,610 --> 00:12:56,490
e sono ad esempio errori nei dati di
addestramento o prompt eccessivamente ambigui,

208
00:12:56,490 --> 00:12:59,769
ma questi sono aspetti su cui è possibile lavorare.

209
00:12:59,769 --> 00:13:04,309
Si possono migliorare i dati fino a farli diventare quasi perfetti,

210
00:13:04,309 --> 00:13:09,830
e si possono migliorare i prompt fino a farli diventare estremamente precisi.

211
00:13:09,830 --> 00:13:10,649
Quello che,

212
00:13:10,649 --> 00:13:11,370
però,

213
00:13:11,370 --> 00:13:18,740
non si può fare è eliminare la necessità di avere un meccanismo
di combinazione dei concetti come quello descritto prima.

214
00:13:18,820 --> 00:13:21,710
Uno studio dell'Università di Singapore,

215
00:13:21,710 --> 00:13:22,409
infatti,

216
00:13:22,409 --> 00:13:29,769
ha dimostrato matematicamente che per quanto possa essere
grande e potente un modello generativo linguistico,

217
00:13:29,769 --> 00:13:35,509
esso non potrà mai imparare tutte le risposte a tutte le domande possibili.

218
00:13:35,509 --> 00:13:41,350
Dovrà pertanto sempre in qualche modo combinare concetti e idee distinte,

219
00:13:41,350 --> 00:13:48,820
e per questo motivo ci sarà sempre una certa
percentuale di rischio che esso produca allucinazioni.

220
00:13:53,750 --> 00:14:01,629
Se le allucinazioni sono forse il tipo di malfunzionamento
più conosciuto nei grandi modelli di generazione linguistica,

221
00:14:01,629 --> 00:14:10,429
esse non sono certo l'unico motivo che ci dovrebbe spingere a mantenere
un certo livello di vigilanza sulle risposte di questi strumenti.

222
00:14:10,429 --> 00:14:13,269
Il fenomeno della psicofantia,

223
00:14:13,269 --> 00:14:14,079
ad esempio,

224
00:14:14,080 --> 00:14:16,160
è venuto alla ribalta di recente,

225
00:14:16,159 --> 00:14:22,829
a seguito di uno specifico aggiornamento di qualche
mese fa del più famoso chatbot in circolazione,

226
00:14:22,830 --> 00:14:25,030
che ovviamente è chatGPT.

227
00:14:25,029 --> 00:14:27,860
Secondo la stessa casa produttrice,

228
00:14:27,860 --> 00:14:28,590
OpenAI,

229
00:14:28,590 --> 00:14:37,129
il chatbot aveva iniziato a bollare come assolutamente
fantastiche le idee più stupide propostegli dagli utilizzatori.

230
00:14:37,129 --> 00:14:46,069
Il culmine si è raggiunto quando un utente si è visto definire "genio"
per aver ideato un business di vendita di cacche su bastoncini.

231
00:14:46,070 --> 00:14:53,270
La definizione psicofante è risultata particolarmente
calzante perché chatGPT aveva iniziato,

232
00:14:53,269 --> 00:14:55,740
in maniera piuttosto evidente,

233
00:14:55,740 --> 00:15:00,039
a favorire le idee degli utenti al di sopra del buonsenso,

234
00:15:00,039 --> 00:15:07,549
e per farlo assumeva perfino un atteggiamento delatorio
nei confronti della realtà dei fatti accertati.

235
00:15:07,549 --> 00:15:13,389
OpenAI ovviamente ha posto rimedio in tempi brevi con un nuovo aggiornamento,

236
00:15:13,389 --> 00:15:15,970
ma la notizia non è passata inosservata,

237
00:15:15,970 --> 00:15:20,200
e anche se la tendenza del bot ad assecondare è stata mitigata,

238
00:15:20,200 --> 00:15:26,390
ciò non vuol dire che ora il modello sia
totalmente immune da questo tipo di comportamenti.

239
00:15:26,389 --> 00:15:26,840
Anzi,

240
00:15:26,840 --> 00:15:31,537
in realtà la cosa più preoccupante risultata poi da una serie di

241
00:15:31,537 --> 00:15:36,307
indagini scatenate dalla notizia è che la psicofantia è risultata

242
00:15:36,307 --> 00:15:40,860
essere molto più comune di quanto si possa pensare nei chatbot,

243
00:15:40,860 --> 00:15:42,830
e anche non così recente.

244
00:15:42,830 --> 00:15:44,590
In un paper di Anthropic,

245
00:15:44,590 --> 00:15:45,470
ad esempio,

246
00:15:45,470 --> 00:15:50,650
altra azienda tra i pesi massimi nella produzione di large language model,

247
00:15:50,649 --> 00:15:56,005
i ricercatori avevano evidenziato già nel 2023 come sia un comportamento

248
00:15:56,005 --> 00:16:00,994
diffuso per gli assistenti digitali quello di sacrificare la realtà

249
00:16:00,994 --> 00:16:05,909
delle cose a favore dell'adattamento al punto di vista dell'utente.

250
00:16:05,909 --> 00:16:11,419
In pratica i chatbot hanno la tendenza a compiacere l'utente di turno,

251
00:16:11,419 --> 00:16:21,389
avvalorando le sue tesi anche se queste sono poco sensate o infondate e
confermando le sue convinzioni anche se imprecise o addirittura sbagliate.

252
00:16:21,389 --> 00:16:21,980
Ora,

253
00:16:21,980 --> 00:16:27,560
vista la celere reazione di OpenAI si potrebbe pensare che la psicofantia,

254
00:16:27,559 --> 00:16:37,039
a differenza delle allucinazioni che abbiamo scoperto essere impossibili
da estirpare a causa della strutturazione stessa del modello linguistico,

255
00:16:37,039 --> 00:16:39,839
sia invece corregibile in qualche modo,

256
00:16:39,840 --> 00:16:43,039
ma purtroppo anche in questo caso non è così,

257
00:16:43,039 --> 00:16:44,929
almeno non per il momento.

258
00:16:44,930 --> 00:16:48,700
La causa di questo strano fenomeno infatti viene fatta

259
00:16:48,700 --> 00:16:52,470
risalire dai ricercatori stessi direttamente al metodo

260
00:16:52,470 --> 00:16:56,240
utilizzato per l'addestramento dei più moderni modelli,

261
00:16:56,240 --> 00:17:00,509
metodo che purtroppo rappresenta anche lo stato dell'arte.

262
00:17:00,509 --> 00:17:05,736
In altre parole non esiste metodo migliore per addestrare un large language

263
00:17:05,736 --> 00:17:10,893
model di quello conosciuto come Reinforcement Learning from Human Feedback

264
00:17:10,893 --> 00:17:15,569
e che viene attualmente utilizzato praticamente per tutti i modelli,

265
00:17:15,569 --> 00:17:17,019
commerciali e non.

266
00:17:17,019 --> 00:17:21,459
Ad oggi l'apprendimento per rinforzo con supervisione umana è l'unico

267
00:17:21,459 --> 00:17:25,518
in grado di portare alla creazione di LLM in grado di competere

268
00:17:25,518 --> 00:17:28,879
o migliorare rispetto a quelli di ultima generazione.

269
00:17:28,880 --> 00:17:33,936
Con la crescita esponenziale del machine learning generativo infatti si è notato

270
00:17:33,936 --> 00:17:38,556
che se da un lato le conoscenze aumentavano in modo più che proporzionale

271
00:17:38,556 --> 00:17:42,240
rispetto ai dati e alle capacità di calcolo a disposizione,

272
00:17:42,240 --> 00:17:46,250
non valeva lo stesso per le capacità di relazionarsi con l'utente.

273
00:17:46,250 --> 00:17:50,238
La comprensione delle richieste ad esempio era scarsa e pertanto

274
00:17:50,238 --> 00:17:54,226
bastava un minimo errore nel prompt per portare alla generazione

275
00:17:54,226 --> 00:17:58,030
di risposte notevolmente lontane dall'argomento della domanda.

276
00:17:58,029 --> 00:18:04,170
I testi prodotti poi risultavano spesso poco
simili a quelli che avrebbe scritto un umano,

277
00:18:04,170 --> 00:18:08,350
sia dal punto di vista della costruzione delle frasi e dei periodi,

278
00:18:08,349 --> 00:18:11,589
sia dal punto di vista della capacità espressiva.

279
00:18:11,589 --> 00:18:19,609
E infine non erano rari atteggiamenti scorretti di vario tipo
verso i più disparati soggetti - affermazioni discriminatorie,

280
00:18:19,610 --> 00:18:20,320
razzismo,

281
00:18:20,319 --> 00:18:21,029
sessismo,

282
00:18:21,029 --> 00:18:22,460
risposte aggressive,

283
00:18:22,460 --> 00:18:27,590
suggerimenti fuori luogo o addirittura
potenzialmente pericolosi e via discorrendo.

284
00:18:27,589 --> 00:18:31,922
Quando quindi le aziende hanno iniziato a realizzare che incrementare

285
00:18:31,922 --> 00:18:35,945
le moli di dati di addestramento e la potenza computazionale non

286
00:18:35,945 --> 00:18:40,029
era più sufficiente ad andare a colmare i limiti dei loro modelli,

287
00:18:40,029 --> 00:18:46,700
hanno iniziato ad adottare l'apprendimento per rinforzo
supervisionato da umani come successivo passo evolutivo.

288
00:18:46,700 --> 00:18:47,870
A onor del vero,

289
00:18:47,869 --> 00:18:52,549
questo non è l'unico motivo che ha portato all'adozione del RLHF.

290
00:18:52,549 --> 00:18:55,339
Ne esiste perlomeno anche un secondo,

291
00:18:55,339 --> 00:18:59,539
anch'esso molto importante e forse anche più intuibile,

292
00:18:59,539 --> 00:19:05,549
ma me lo lascio per il prossimo blocco perché
ci aiuterà a capire meglio un certo concetto.

293
00:19:05,549 --> 00:19:06,470
Ad ogni modo,

294
00:19:06,470 --> 00:19:07,089
però,

295
00:19:07,089 --> 00:19:09,329
una volta risolto un problema,

296
00:19:09,329 --> 00:19:10,589
come spesso accade,

297
00:19:10,589 --> 00:19:15,710
se ne è venuto a creare un altro strettamente legato alla soluzione adottata.

298
00:19:15,710 --> 00:19:22,170
Il reinforcement learning con supervisione
umana funziona con una semplice logica.

299
00:19:22,170 --> 00:19:24,690
Il modello viene messo al lavoro,

300
00:19:24,690 --> 00:19:29,120
quindi nello specifico gli vengono fatte domande a cui deve rispondere,

301
00:19:29,119 --> 00:19:36,399
e viene premiato o punito a seconda che il
controllore consideri buona o cattiva la risposta.

302
00:19:36,400 --> 00:19:42,970
Nel reinforcement learning semplice le punizioni
e i premi sono risultati di formule matematiche,

303
00:19:42,970 --> 00:19:46,740
ma di questo abbiamo già parlato nell'episodio su Alphadev,

304
00:19:46,740 --> 00:19:50,650
che ti lascio sempre in descrizione e che ti invito a recuperare.

305
00:19:50,650 --> 00:19:53,750
Nel metodo con rinforzo umano,

306
00:19:53,750 --> 00:19:54,309
invece,

307
00:19:54,309 --> 00:20:03,339
la differenza principale sta nel fatto che il premio o la punizione viene
assegnato non da un calcolo matematico ma direttamente dal giudizio umano.

308
00:20:03,339 --> 00:20:07,449
Questa affermazione suona come un enorme passo avanti,

309
00:20:07,450 --> 00:20:09,000
ed in effetti lo è,

310
00:20:09,000 --> 00:20:14,509
ma nasconde anche un'insidia che ad una prima occhiata non è così evidente.

311
00:20:14,509 --> 00:20:15,180
Se prima,

312
00:20:15,180 --> 00:20:15,769
infatti,

313
00:20:15,769 --> 00:20:20,559
i modelli venivano addestrati solo tramite rigorose formule matematiche,

314
00:20:20,559 --> 00:20:24,473
si poteva in un certo qual modo fare affidamento sul fatto

315
00:20:24,473 --> 00:20:27,989
che il risultato dell'operazione sarebbe stato tanto

316
00:20:27,989 --> 00:20:31,240
affidabile quanto lo erano le formule utilizzate.

317
00:20:31,240 --> 00:20:32,000
Invece,

318
00:20:32,000 --> 00:20:36,549
inserendo una fase svolta da umani all'interno del processo,

319
00:20:36,549 --> 00:20:39,159
questa sicurezza viene un po' a decadere.

320
00:20:39,160 --> 00:20:43,210
Ovviamente anche in questo caso il modello viene ottimizzato,

321
00:20:43,210 --> 00:20:46,400
ma non necessariamente nel migliore dei modi.

322
00:20:46,400 --> 00:20:48,620
Dato infatti che gli umani,

323
00:20:48,619 --> 00:20:50,159
purtroppo o per fortuna,

324
00:20:50,160 --> 00:20:51,380
sono fallibili,

325
00:20:51,380 --> 00:20:57,180
lo è anche il giudizio che possono esprimere sulle risposte generate da un LLM.

326
00:20:57,180 --> 00:20:59,240
Statisticamente parlando,

327
00:20:59,240 --> 00:21:08,110
non è pensabile che tutti i controllori conoscano alla perfezione
e siano in grado di etichettare in modo perfetto ogni risposta.

328
00:21:08,110 --> 00:21:18,130
Capita che essi possano dare il proprio giudizio anche rispetto ad
output che sembrano giusti o sbagliati ma in realtà non lo sono.

329
00:21:18,130 --> 00:21:19,170
Sappiamo bene,

330
00:21:19,170 --> 00:21:21,430
l'abbiamo detto già più volte,

331
00:21:21,430 --> 00:21:25,570
che gli LLM hanno un modo molto convincente di esporre le informazioni,

332
00:21:25,569 --> 00:21:27,869
giuste o sbagliate che siano.

333
00:21:27,869 --> 00:21:35,379
Gli umani sono umani e pertanto tendono a farsi
convincere anche da determinati modi di fare,

334
00:21:35,380 --> 00:21:39,290
da determinati toni nell'esposizione e perché no,

335
00:21:39,289 --> 00:21:42,089
anche da lusinghe a condiscendenza,

336
00:21:42,089 --> 00:21:43,899
che in maniera velata o meno,

337
00:21:43,900 --> 00:21:46,730
possono comunque solleticare il loro ego.

338
00:21:46,730 --> 00:21:49,230
Dal canto suo invece il modello,

339
00:21:49,230 --> 00:21:50,589
ancora una volta,

340
00:21:50,589 --> 00:21:57,559
non è dotato di alcun tipo di capacità di comprendere
cosa produce in output e cosa riceve in input.

341
00:21:57,559 --> 00:22:03,899
Pertanto non sa per quale motivo le risposte gli
vengono contrassegnate come giuste o sbagliate,

342
00:22:03,900 --> 00:22:05,410
sa solo che è così.

343
00:22:05,410 --> 00:22:17,200
Con l'avanzare del processo quindi esso accumula una serie di dati e cerca di
estrapolarne un qualcosa che accomuni tra loro le risposte giuste da una parte,

344
00:22:17,200 --> 00:22:21,140
quelle sbagliate dall'altra e tutte le sfumature nel mezzo.

345
00:22:21,139 --> 00:22:22,289
In definitiva,

346
00:22:22,289 --> 00:22:26,000
con il reinforcement learning by human feedback,

347
00:22:26,000 --> 00:22:31,679
un importante effetto collaterale è che il modello impara fra le varie cose,

348
00:22:31,679 --> 00:22:37,869
anche che le risposte migliori sono quelle che
maggiormente assecondano il proprio controllore,

349
00:22:37,869 --> 00:22:40,569
magari quelle che gli danno ragione o che,

350
00:22:40,570 --> 00:22:41,300
comunque,

351
00:22:41,299 --> 00:22:42,590
non gli danno torto.

352
00:22:47,319 --> 00:22:57,529
L'ultima frontiera nel campo dei grandi modelli
linguistici sono i cosiddetti Large Resonance Model,

353
00:22:57,529 --> 00:22:59,079
o LRM all'italiana.

354
00:22:59,079 --> 00:23:00,759
Dicendola in modo semplice,

355
00:23:00,759 --> 00:23:02,750
essi sono dei modelli che,

356
00:23:02,750 --> 00:23:05,970
per rispondere ad un determinato prompt,

357
00:23:05,970 --> 00:23:08,460
eseguono tutta una serie di passaggi,

358
00:23:08,460 --> 00:23:11,920
effettuando delle domande a se stessi e rispondendovi,

359
00:23:11,920 --> 00:23:17,400
per migliorare la comprensione del problema
sottoposto e la qualità della risposta.

360
00:23:17,400 --> 00:23:25,100
Questa serie di scambi precedenti alla generazione
dell'output viene definita Chain of Thought,

361
00:23:25,099 --> 00:23:26,659
catena di pensiero,

362
00:23:26,660 --> 00:23:32,590
e in molti modelli viene proprio esplicitata come parte della risposta stessa.

363
00:23:32,589 --> 00:23:38,949
Ciò permette agli utenti di verificare in che modo il
modello sia arrivato a formulare l'output fornito.

364
00:23:38,950 --> 00:23:42,569
Peccato però che un recente studio,

365
00:23:42,569 --> 00:23:44,669
sempre di Antropic,

366
00:23:44,670 --> 00:23:47,210
ha evidenziato come in realtà,

367
00:23:47,210 --> 00:23:51,000
in una percentuale sorprendentemente alta di casi,

368
00:23:51,000 --> 00:23:58,200
gli LRM presi in esame tendono a divergere
tra la risposta e la catena di pensiero.

369
00:23:58,200 --> 00:24:07,460
Gli esperimenti riportati si basavano essenzialmente sul fornire ai
modelli dei suggerimenti da utilizzare per rispondere alle domande,

370
00:24:07,460 --> 00:24:12,130
e poi sull'andare a verificare se essi venivano innanzitutto utilizzati,

371
00:24:12,130 --> 00:24:15,040
e poi menzionati nella Chain of Thought.

372
00:24:15,039 --> 00:24:18,259
Tali suggerimenti potevano essere di vario tipo,

373
00:24:18,259 --> 00:24:19,940
alcuni erano corretti,

374
00:24:19,940 --> 00:24:21,350
altri sbagliati,

375
00:24:21,349 --> 00:24:22,559
alcuni palesi,

376
00:24:22,559 --> 00:24:24,289
altri poco evidenti,

377
00:24:24,289 --> 00:24:31,240
e al modello veniva lasciata piena libertà di
scelta nel decidere se utilizzarli o meno.

378
00:24:31,240 --> 00:24:35,839
L'idea alla base degli esperimenti era quella di verificare se il modello,

379
00:24:35,839 --> 00:24:38,599
pur decidendo di utilizzare il suggerimento,

380
00:24:38,599 --> 00:24:42,480
evitasse poi di menzionarlo nella catena di pensiero.

381
00:24:42,480 --> 00:24:45,690
In tal caso sarebbe stato considerato bugiardo,

382
00:24:45,690 --> 00:24:47,600
in caso contrario onesto.

383
00:24:47,599 --> 00:24:54,639
I risultati hanno mostrato che la maggioranza
sostanziale delle risposte è stata infedele.

384
00:24:54,640 --> 00:24:56,690
Tra i vari modelli esaminati,

385
00:24:56,690 --> 00:24:57,600
ad esempio,

386
00:24:57,599 --> 00:25:04,639
in media Cloud 3.7 Sonnet ha menzionato il suggerimento solo il 25% delle volte,

387
00:25:04,639 --> 00:25:09,959
mentre DeepSeek R1 lo ha menzionato appena il 39% delle volte.

388
00:25:09,960 --> 00:25:11,210
Nel complesso,

389
00:25:11,210 --> 00:25:16,516
la ricerca indica che i modelli di ragionamento avanzati in generale molto

390
00:25:16,516 --> 00:25:21,539
spesso nascondono i loro veri processi di pensiero e talvolta lo fanno

391
00:25:21,539 --> 00:25:26,279
proprio quando i loro comportamenti sono esplicitamente divergenti.

392
00:25:26,279 --> 00:25:31,009
In pratica possiamo dire che gli LRM di fatto mentono.

393
00:25:31,009 --> 00:25:34,660
E non intendendo che essi riportano informazioni errate,

394
00:25:34,660 --> 00:25:38,200
quello l'abbiamo già constatato parlando di allucinazioni,

395
00:25:38,200 --> 00:25:42,279
ma proprio nel senso che essi non dicono quello che pensano.

396
00:25:42,279 --> 00:25:52,839
Ovviamente dopo un'affermazione del genere è importante ricordare che
un LLM come un LRM non dice verità o menzogna con intenzionalità.

397
00:25:52,839 --> 00:25:55,919
Come abbiamo già detto nei blocchi precedenti,

398
00:25:55,920 --> 00:26:02,000
tira semplicemente fuori la sequenza di
parole più probabili collegate al prompt.

399
00:26:02,000 --> 00:26:06,292
Prima però ti ho anche anticipato che c'è una seconda ragione per la

400
00:26:06,292 --> 00:26:10,772
quale il reinforcement learning con feedback umano è stato adottato per

401
00:26:10,772 --> 00:26:15,190
l'addestramento dei più avanzati modelli linguistici e di ragionamento,

402
00:26:15,190 --> 00:26:17,720
ed è arrivato il momento di parlarne.

403
00:26:17,720 --> 00:26:22,403
In uno scenario in espansione competitivo come quello della

404
00:26:22,403 --> 00:26:26,930
IA generativa non dobbiamo mai dimenticarci che LLM o LRM

405
00:26:26,930 --> 00:26:31,380
sono macchine appositamente progettate per dare risposte.

406
00:26:31,380 --> 00:26:38,070
Pertanto la possibilità che esse ammettano
di non saper rispondere non è auspicabile.

407
00:26:38,069 --> 00:26:42,970
Le aziende per loro natura devono massimizzare il profitto e,

408
00:26:42,970 --> 00:26:47,710
in quanto prodotto impiegato per la generazione di fatturato,

409
00:26:47,710 --> 00:26:54,009
la risposta "non lo so da parte di un modello
generativo" viene di fatto considerata un problema.

410
00:26:54,009 --> 00:26:58,939
Un sistema che funziona a intermittenza o non genera senso di sicurezza

411
00:26:58,939 --> 00:27:03,869
negli utenti che pagano fior fiore di quattrini per costruirvi sopra le

412
00:27:03,869 --> 00:27:07,293
proprie soluzioni tecnologiche di ogni tipo non è

413
00:27:07,293 --> 00:27:10,579
esattamente una manna dal cielo per il business.

414
00:27:10,579 --> 00:27:14,698
Come abbiamo già detto però già da un po' ci si è resi conto che

415
00:27:14,698 --> 00:27:18,564
il solo continuare ad aumentare parametri e dati non sarebbe

416
00:27:18,564 --> 00:27:22,619
stato sufficiente a produrre macchine onniscienti e infallibili.

417
00:27:22,619 --> 00:27:31,319
Ed è proprio per questo che il nostro discorso ritorna sul
reinforcement learning basato su feedback umano che è,

418
00:27:31,319 --> 00:27:32,559
ripetiamolo,

419
00:27:32,559 --> 00:27:39,319
lo stato dell'arte dell'addestramento e permette agli
sviluppatori di produrre modelli sempre più efficienti.

420
00:27:39,319 --> 00:27:50,309
Peccato però che come qualsiasi large language model o large reasoning
model là fuori nemmeno gli umani siano poi così onniscienti e infallibili.

421
00:27:50,309 --> 00:27:55,189
Come abbiamo già detto infatti un grosso limite del RLHF è che

422
00:27:55,189 --> 00:28:00,455
tende sì ad ottimizzare le capacità di risposta delle IA tramite la

423
00:28:00,455 --> 00:28:05,799
massimizzazione del premio ma non sempre lo fa nel migliore dei modi.

424
00:28:05,799 --> 00:28:10,712
Innanzitutto dato che difficilmente un controllore si accontenterà di

425
00:28:10,712 --> 00:28:15,765
una risposta del tipo "sinceramente a questa domanda non so rispondere"

426
00:28:15,765 --> 00:28:20,607
una cosa fondamentale che l'addestramento supervisionato imprime nei

427
00:28:20,607 --> 00:28:25,379
modelli è che "non lo so" non è una risposta che viene ricompensata.

428
00:28:25,380 --> 00:28:30,799
Al tempo stesso però gli umani designati come controllori ovviamente non possono

429
00:28:30,799 --> 00:28:36,152
conoscere ogni singola nozione dello scibile umano e altrettanto ovviamente non

430
00:28:36,152 --> 00:28:39,565
sono in grado di giudicare tutte le risposte della

431
00:28:39,565 --> 00:28:42,710
macchina con precisione totale e infallibilità.

432
00:28:42,710 --> 00:28:47,125
Ciò vuol dire che durante l'addestramento quando il modello

433
00:28:47,125 --> 00:28:51,321
genera una risposta che viene contrassegnata come errata

434
00:28:51,321 --> 00:28:55,590
ci sono di fatto due strade per ottimizzare la situazione.

435
00:28:55,589 --> 00:29:01,879
La prima è migliorare nel dare risposte più dettagliate e corrette ma l'altra,

436
00:29:01,880 --> 00:29:04,310
ed è qui che si verifica il problema,

437
00:29:04,309 --> 00:29:12,559
è generare risposte più convincenti e che
sembrino più corrette pur non essendolo.

438
00:29:12,559 --> 00:29:25,109
Se dunque l'LRM non ha una risposta corretta e "non lo so" non è una risposta
valida allora quello che resta è generare una risposta abbastanza verosimile,

439
00:29:25,110 --> 00:29:33,600
ben strutturata e in forma fluente da riuscire a superare
il controllo anche senza essere necessariamente corretta.

440
00:29:33,599 --> 00:29:37,871
I supervisori umani infatti semplicemente possono non essere in

441
00:29:37,871 --> 00:29:42,409
grado di segnalare come risposte sbagliate quelle che pur essendolo

442
00:29:42,409 --> 00:29:46,279
appaiono però abbastanza sensate e coerenti da ingannarli.

443
00:29:46,279 --> 00:29:49,656
L'obiettivo dell'addestramento quindi diventa non

444
00:29:49,656 --> 00:29:53,235
più quello di formulare risposte esatte ma quello di

445
00:29:53,235 --> 00:29:56,950
convincere l'essere umano che la risposta sia corretta,

446
00:29:56,950 --> 00:29:58,039
non importa come,

447
00:29:58,039 --> 00:30:02,759
se migliorando veramente l'output o semplicemente facendoglielo credere.

448
00:30:02,759 --> 00:30:12,480
In pratica il modello impara che migliorare nelle capacità di nascondere
l'incompetenza funziona altrettanto bene che migliorare nella conoscenza.

449
00:30:12,480 --> 00:30:19,360
Di fatto lei ha mentono perché noi stiamo
dicendo loro che facendolo vengono ricompensate.

450
00:30:19,360 --> 00:30:25,312
In uno studio pubblicato su Nature alcuni ricercatori hanno evidenziato come con

451
00:30:25,312 --> 00:30:31,044
il passare delle generazioni dei modelli generativi le risposte del tipo "non

452
00:30:31,044 --> 00:30:34,719
lo so" sono state progressivamente sostituite con

453
00:30:34,719 --> 00:30:38,320
risposte molto più articolate ma anche sbagliate.

454
00:30:38,319 --> 00:30:42,411
A quanto pare più difficile la domanda sottoposta e più

455
00:30:42,411 --> 00:30:46,283
avanzato è il modello utilizzato più è probabile che

456
00:30:46,283 --> 00:30:50,009
la risposta generata sia un insieme di sciocchezze,

457
00:30:50,009 --> 00:30:53,639
solo che saranno sciocchezze molto plausibili e molto ben scritte.

458
00:30:53,639 --> 00:31:03,559
Gli stessi ricercatori poi hanno anche svolto un sondaggio online con
300 partecipanti per capire quale modello fosse il più bravo a mentire.

459
00:31:03,559 --> 00:31:07,990
Il vincitore è risultato essere ChatGPT ma la cosa a mio

460
00:31:07,990 --> 00:31:12,808
avviso più interessante è che le persone non sono riuscite ad

461
00:31:12,808 --> 00:31:17,549
individuare gli errori in una percentuale molto alta di casi.

462
00:31:17,549 --> 00:31:26,619
Tra le varie le risposte errate fornite nella categoria scientifica
sono state qualificate come corrette in oltre il 19% di casi,

463
00:31:26,619 --> 00:31:31,559
quelle di geografia nel 32 e addirittura le trasformazioni,

464
00:31:31,559 --> 00:31:38,400
cioè compiti in cui si chiedeva di estrarre e
riorganizzare le informazioni presenti nei prompt,

465
00:31:38,400 --> 00:31:40,760
in ben il 40%.

466
00:31:40,759 --> 00:31:51,519
In pratica stiamo insegnando alle intelligenze artificiali
a mentire e lo stiamo anche facendo molto bene.

467
00:31:52,250 --> 00:31:57,450
Sapere come funzionano certe limitazioni delle intelligenze artificiali,

468
00:31:57,450 --> 00:32:02,190
strumenti che ormai utilizziamo praticamente ogni giorno,

469
00:32:02,190 --> 00:32:08,130
è fondamentale per evitare di incappare in una
serie di incidenti quanto meno spiacevoli.

470
00:32:08,130 --> 00:32:09,810
Tanto per cominciare,

471
00:32:09,809 --> 00:32:11,289
è lo stesso Sam Altman,

472
00:32:11,289 --> 00:32:12,529
CEO di OpenAI,

473
00:32:12,529 --> 00:32:17,649
a ricordare sempre più spesso al mondo che il suo prodotto di punta,

474
00:32:17,650 --> 00:32:18,590
ChatGPT,

475
00:32:18,589 --> 00:32:28,959
soffre costantemente di allucinazioni e a chiedersi come sia possibile
che tante persone credano ciecamente a qualsiasi risposta esso produca.

476
00:32:28,960 --> 00:32:30,130
E se lo dice lui,

477
00:32:30,130 --> 00:32:30,890
voglio dire.

478
00:32:30,890 --> 00:32:36,210
Nell'ultimo articolo che mi è capitato sotto mano c'è questo virgolettato,

479
00:32:36,210 --> 00:32:37,809
la traduzione è mia.

480
00:32:37,809 --> 00:32:43,490
"Le persone hanno un livello molto alto di fiducia in ChatGPT,

481
00:32:43,490 --> 00:32:48,009
il che è interessante perché l'AI ha allucinazioni,

482
00:32:48,009 --> 00:32:53,009
dovrebbe essere quella tecnologia di cui non ti fidi così tanto".

483
00:32:53,009 --> 00:32:54,490
Fine citazione.

484
00:32:54,490 --> 00:32:58,650
Le problematiche che abbiamo descritto in questo episodio,

485
00:32:58,650 --> 00:33:03,550
come spesso accade per gli argomenti di cui parliamo in questo podcast,

486
00:33:03,549 --> 00:33:08,750
sono virtuali ma hanno poi ricadute estremamente reali.

487
00:33:08,750 --> 00:33:11,630
Per la questione delle allucinazioni,

488
00:33:11,630 --> 00:33:14,690
non penso di doverti chiarire chissà cosa.

489
00:33:14,690 --> 00:33:16,210
Gli LLM sbagliano,

490
00:33:16,210 --> 00:33:19,190
possono tirare fuori informazioni sbagliate,

491
00:33:19,190 --> 00:33:21,170
possono sbagliare i calcoli,

492
00:33:21,170 --> 00:33:23,970
possono riportare male tendenze,

493
00:33:23,970 --> 00:33:26,970
andamenti o fare analisi incongrue.

494
00:33:26,970 --> 00:33:32,900
E non parliamo poi dei collegamenti logici fra concetti e ragionamenti.

495
00:33:32,900 --> 00:33:33,769
In pratica,

496
00:33:33,769 --> 00:33:37,069
se li utilizziamo per prendere decisioni,

497
00:33:37,069 --> 00:33:41,230
per fare scelte o per creare testi da utilizzare altrove,

498
00:33:41,230 --> 00:33:46,190
dalla stesura di un libro alla preparazione di biglietti di auguri,

499
00:33:46,190 --> 00:33:53,559
dobbiamo essere consci del fatto che in ogni
singola frase si potrebbero annidare degli errori.

500
00:33:53,559 --> 00:33:58,289
Questo chiaramente non significa che non li possiamo usare mai.

501
00:33:58,289 --> 00:33:59,289
Anzi,

502
00:33:59,289 --> 00:34:01,950
io ne incoraggio l'utilizzo quando utile.

503
00:34:01,950 --> 00:34:02,930
Mi raccomando,

504
00:34:02,930 --> 00:34:05,690
non credere che io sia un luddista,

505
00:34:05,690 --> 00:34:07,309
sono il primo che li usa.

506
00:34:07,309 --> 00:34:12,509
A seconda del contesto e dell'importanza dell'attività che stiamo svolgendo,

507
00:34:12,510 --> 00:34:13,030
però,

508
00:34:13,030 --> 00:34:20,220
dobbiamo tenere bene a mente che è necessario
controllare a fondo i loro output prima di utilizzarli.

509
00:34:20,219 --> 00:34:21,250
Diversamente,

510
00:34:21,250 --> 00:34:21,860
invece,

511
00:34:21,860 --> 00:34:27,119
se spostiamo l'attenzione sulla psicofantia che abbiamo descritto poc'anzi,

512
00:34:27,119 --> 00:34:33,099
i problemi sono forse un po' meno evidenti
rispetto a quelli delle generiche allucinazioni.

513
00:34:33,099 --> 00:34:38,483
Al di là del semplice fatto di trovare magari fastidioso un interlocutore al

514
00:34:38,483 --> 00:34:43,657
quale stiamo in effetti chiedendo aiuto per portare a termine un'attività

515
00:34:43,657 --> 00:34:46,943
e che continua a risponderci riportando quanto

516
00:34:46,943 --> 00:34:50,090
le nostre idee siano intelligenti e perfette,

517
00:34:50,090 --> 00:34:59,390
l'artificioso modo di assecondare degli LLM potrebbe in
realtà rappresentare una sorta di trappola per l'utente.

518
00:34:59,389 --> 00:35:01,480
Come già accaduto per i social,

519
00:35:01,480 --> 00:35:06,710
i quali hanno spinto all'estremo la proliferazione delle cosiddette bolle,

520
00:35:06,710 --> 00:35:16,809
cioè quelle sorte di ambienti protetti in cui noi tutti trascorriamo
parte del nostro tempo interagendo con persone che la pensano come noi,

521
00:35:16,809 --> 00:35:21,569
o a contatto con informazioni che descrivono il mondo come noi lo vediamo,

522
00:35:21,570 --> 00:35:26,470
o ragionamenti che si allineano alle nostre convinzioni,

523
00:35:26,469 --> 00:35:34,849
allo stesso modo un chatbot psicofante diventa un meccanismo
di rinforzo del nostro pensiero e dei nostri bias.

524
00:35:34,849 --> 00:35:38,349
Se nessuno mi dice mai che ho torto su qualcosa,

525
00:35:38,349 --> 00:35:40,750
che sto sbagliando un ragionamento,

526
00:35:40,750 --> 00:35:43,969
o sono in possesso di un'informazione sbagliata,

527
00:35:43,969 --> 00:35:47,669
o ho fatto un ragionamento campato in aria,

528
00:35:47,670 --> 00:35:52,289
ma anzi si complimenta per qualsiasi cosa io esterni,

529
00:35:52,289 --> 00:36:00,849
allora non mi metto in discussione e mi convinco di essere
totalmente nel giusto e che tutto il mondo la pensa come me,

530
00:36:00,849 --> 00:36:02,559
ma questo non è vero,

531
00:36:02,559 --> 00:36:03,730
non lo è mai.

532
00:36:03,730 --> 00:36:08,449
Se i chatbot divengono ennesimi strumenti di rinforzo della bolla,

533
00:36:08,449 --> 00:36:15,779
invece di aiutarmi a scoprire ed accettare diversi punti
di vista o imparare a gestire il confronto e gli errori,

534
00:36:15,779 --> 00:36:25,209
saranno semplicemente una nuova gabbia dorata che mi isolerà
ancora una volta dal mondo reale e da tutte le sue sfaccettature.

535
00:36:25,210 --> 00:36:26,250
Infine,

536
00:36:26,250 --> 00:36:30,769
nel nostro discorso restano quelle che abbiamo definito come menzogne,

537
00:36:30,769 --> 00:36:36,029
cioè il nascondere la vera catena di pensieri che ha portato ad una risposta.

538
00:36:36,029 --> 00:36:41,019
Esse sono una caratteristica degli LRM che ha vari impatti,

539
00:36:41,019 --> 00:36:43,510
soprattutto a livello tecnico.

540
00:36:43,510 --> 00:36:44,790
Innanzitutto,

541
00:36:44,789 --> 00:36:50,576
le catene di pensiero vengono utilizzate per studiare il comportamento dei

542
00:36:50,576 --> 00:36:56,208
modelli e verificare come avviene la produzione di un determinato output

543
00:36:56,208 --> 00:37:01,300
e pertanto la loro inesattezza inficia le attività di ricercatori,

544
00:37:01,300 --> 00:37:03,210
studiosi e progettisti.

545
00:37:03,210 --> 00:37:06,460
Ma pensiamo a qualcosa di ancora più pratico.

546
00:37:06,460 --> 00:37:08,389
Facciamo un esempio.

547
00:37:08,389 --> 00:37:12,796
Immagina che ad un modello di ragionamento per il supporto medico

548
00:37:12,796 --> 00:37:16,870
venga chiesto quale delle seguenti indicazioni dobbiamo dare

549
00:37:16,870 --> 00:37:21,010
ad un paziente per ridurre il rischio di sviluppare un tumore,

550
00:37:21,010 --> 00:37:23,090
eliminare la carne rossa,

551
00:37:23,090 --> 00:37:24,660
i grassi alimentari,

552
00:37:24,659 --> 00:37:27,049
il pesce o curare l'obesità.

553
00:37:27,050 --> 00:37:28,490
Il modello vede,

554
00:37:28,489 --> 00:37:34,209
da qualche parte nell'intera storia clinica
del paziente o di chissà quale altro archivio,

555
00:37:34,210 --> 00:37:38,264
un sottile indizio che indica che la risposta corretta è

556
00:37:38,264 --> 00:37:42,746
"eliminare i grassi alimentari" e scrive una lunga spiegazione

557
00:37:42,746 --> 00:37:46,730
nella sua catena di pensiero sul perché essa è corretta,

558
00:37:46,730 --> 00:37:51,170
senza mai menzionare di aver utilizzato quell'indizio.

559
00:37:51,170 --> 00:37:52,840
In un caso del genere,

560
00:37:52,840 --> 00:38:00,730
c'è solo da sperare di essere in presenza di un bravo medico
che si accorga che lo strumento ha generato un'allucinazione,

561
00:38:00,730 --> 00:38:04,889
perché dalle informazioni fornite sul processo di ragionamento effettuato,

562
00:38:04,889 --> 00:38:10,429
sarà di fatto impossibile individuare la presenza dell'errore e la sua fonte.

563
00:38:10,429 --> 00:38:11,730
In conclusione,

564
00:38:11,730 --> 00:38:18,610
i chatbot basati su intelligenza artificiale generativa
sono inaffidabili sotto molti punti di vista,

565
00:38:18,610 --> 00:38:26,840
anche se si sta lavorando per migliorarli e ci sono
tante novità e passi avanti praticamente ogni giorno.

566
00:38:26,840 --> 00:38:30,590
Forse un giorno i problemi verranno risolti,

567
00:38:30,590 --> 00:38:32,860
ma per il momento non è così,

568
00:38:32,860 --> 00:38:36,340
e secondo me serviranno ancora un bel po' di anni.

569
00:38:36,340 --> 00:38:37,380
Nel frattempo,

570
00:38:37,380 --> 00:38:37,980
quindi,

571
00:38:37,980 --> 00:38:39,170
quello che faccio,

572
00:38:39,170 --> 00:38:41,260
e che consiglio di fare anche a te,

573
00:38:41,260 --> 00:38:44,210
è fare attenzione a controllare gli output.

574
00:38:44,210 --> 00:38:49,559
So che non possiamo vigilare su tutto e verificare ogni singola parola,

575
00:38:49,559 --> 00:38:54,009
altrimenti invece di farci risparmiare tempo ce ne farebbero perdere.

576
00:38:54,010 --> 00:38:54,480
Però,

577
00:38:54,480 --> 00:38:57,650
almeno per le cose che riteniamo importanti,

578
00:38:57,650 --> 00:38:58,070
beh,

579
00:38:58,070 --> 00:39:00,130
lo sforzo dovremmo farlo.

580
00:39:00,130 --> 00:39:01,150
D'altronde,

581
00:39:01,150 --> 00:39:06,090
nessuna tecnologia funziona senza alcun costo da pagare e,

582
00:39:06,090 --> 00:39:09,230
nel caso dell'intelligenza generativa,

583
00:39:09,230 --> 00:39:15,690
la vigilanza è una delle componenti del prezzo più importanti e meno evidenti.

584
00:39:20,570 --> 00:39:22,080
Ce l'abbiamo fatta!

585
00:39:22,079 --> 00:39:25,250
Abbiamo portato a casa l'episodio 142.

586
00:39:25,250 --> 00:39:27,849
Ultimamente sono molto in difficoltà.

587
00:39:27,849 --> 00:39:28,460
Devo dire,

588
00:39:28,460 --> 00:39:32,329
magari dovrei provare a fare un ragionamento approfondito sul perché,

589
00:39:32,329 --> 00:39:33,110
ma non oggi,

590
00:39:33,110 --> 00:39:36,329
perché altrimenti questo podcast non esce più.

591
00:39:36,329 --> 00:39:41,519
Velocissimamente ringrazio i donatori periodici Edoardo e Carlo,

592
00:39:41,519 --> 00:39:46,969
e poi Michele e Paola che si aggiungono oggi con la loro donazione singola.

593
00:39:46,969 --> 00:39:51,730
Loro hanno scelto di restituire valore a Pensieri in Codice in questo modo.

594
00:39:51,730 --> 00:39:53,610
Se vuoi farlo anche tu,

595
00:39:53,610 --> 00:40:00,740
trovi i link nella descrizione e nella sezione
Sostieni del sito pensierincodice.it Ricorda,

596
00:40:00,739 --> 00:40:04,609
non è obbligatorio e non c'è una cifra minima.

597
00:40:04,610 --> 00:40:08,789
La scelta di quanto vale il mio lavoro io la lascio a te.

598
00:40:08,789 --> 00:40:14,969
Se invece preferisci ricompensarmi con un
po' del tuo talento o un po' del tuo tempo,

599
00:40:14,969 --> 00:40:19,369
ricordati che portare nuovi ascoltatori è sempre un bel modo.

600
00:40:19,369 --> 00:40:24,589
Nel 2025 non ti devo certo spiegare io come diffondere un contenuto,

601
00:40:24,590 --> 00:40:25,309
giusto?

602
00:40:25,309 --> 00:40:30,400
E poi ricorda anche che ci sono tante attività su cui puoi dare una mano.

603
00:40:30,400 --> 00:40:32,420
Contattami e parliamone.

604
00:40:32,420 --> 00:40:37,420
C'è ad esempio la nuova rubrica Pensieri in Codice Community Edition,

605
00:40:37,420 --> 00:40:40,630
in cui puoi creare il tuo episodio con la tua voce.

606
00:40:40,630 --> 00:40:44,730
Oppure c'è la necessità di aprire e gestire degli account social.

607
00:40:44,730 --> 00:40:45,320
Oppure,

608
00:40:45,320 --> 00:40:46,490
come dico sempre,

609
00:40:46,489 --> 00:40:50,959
dimmi tu cosa sapresti o vorresti fare e vediamo di organizzarci.

610
00:40:50,960 --> 00:40:54,210
Mi trovi su Telegram nel gruppo Pensieri in Codice,

611
00:40:54,210 --> 00:41:02,320
sempre link in descrizione e sul sito pensierincodice.it o
all'indirizzo valerio@pensieriincodice.it Mi raccomando,

612
00:41:02,320 --> 00:41:03,330
con due i.

613
00:41:03,329 --> 00:41:04,130
Infine,

614
00:41:04,130 --> 00:41:07,680
per citare il mio caro amico Alex Racuglia,

615
00:41:07,679 --> 00:41:12,319
che ultimamente ha realizzato ben quattro episodi per Community Edition,

616
00:41:12,320 --> 00:41:18,400
ti auguro un buon ascolto di quel che verrà e
noi ci risentiamo probabilmente a settembre,

617
00:41:18,400 --> 00:41:23,180
senza mai dimenticarci però che un informatico risolve problemi,

618
00:41:23,179 --> 00:41:25,449
a volte anche usando il computer!

